{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cf56c7-b6e3-4824-a0c9-bedca391557f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Slurm Job Scheduling \n",
    "\n",
    "When working on a multi-user HPC system (like a cluster or supercomputer), you typically **don't run big GPU jobs directly on the login node**. Instead, you use a scheduler to queue up your work. **Slurm** (Simple Linux Utility for Resource Management) is a widely used job scheduler for HPC environments. It manages allocating computer resources (CPUs, GPUs, memory) to use jobs and queues them if the cluster is busy. \n",
    "\n",
    "This section will introduce **Slurm** for beginners, covering how to submit and manage jobs using commands like `sbatch`, `squeue`, and `scancel`, and how to set up job dependencies so jobs run in a certain order. \n",
    "\n",
    "## What is a Slurm job?\n",
    "A *job* in Slurm is a unit of work, usually defined by a **job script**. The job script is a bash script (or another shell) that specifies resources needed (via special `#SBATCH` directives) and the commands to execute. When you submit this script, Slurm will find an available compute node (or nodes) that meet your requirements (CPU cores, GPUs, time, etc.) and run the script there, not on the login machine.\n",
    "\n",
    "## Creating a Simple Job Script \n",
    "Here's a fundamental example of a Slurm job script, which we could call `myjob.slurm`: \n",
    "\n",
    "```bash \n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=testjob          # Name of the job\n",
    "#SBATCH --output=job_%j.out         # Output file (%j will be replaced with job ID)\n",
    "#SBATCH --error=job_%j.err          # Error file (a separate file for stderr, optional)\n",
    "#SBATCH --time=0-00:05              # Wall time (DD-HH:MM) here 5 minutes\n",
    "#SBATCH --partition=gpu             # Partition/queue name, e.g., 'gpu' or as configured\n",
    "#SBATCH --gres=gpu:1                # Request 1 GPU (generic resource)\n",
    "#SBATCH --cpus-per-task=4           # Request 4 CPU cores\n",
    "#SBATCH --mem=16G                   # Request 16 GB of RAM\n",
    "\n",
    "echo \"Hello from job $SLURM_JOB_ID running on $SLURM_NODELIST\"\n",
    "sleep 60  # Simulate work by sleeping for 60 seconds\n",
    "```\n",
    "\n",
    "The script uses `#SBATCH` lines to request resources: \n",
    "- A job name for easy identification. \n",
    "- Output/error file names.\n",
    "- A time limit of 5 minutes. \n",
    "- The partition (queue) to run on (often clusters have a special partition for GPU jobs) \n",
    "- 1 GPU(`--gres=gpu:1` means one GPU)\n",
    "- 4 CPU cores and 16GB of memory. \n",
    "\n",
    "The body of the script prints a message and sleeps 60 seconds (as a placeholder for real work). `SLURM_JOB_ID` and `$SLUM_NODELIST` are environment variables Slurm sets for your job. \n",
    "\n",
    "## Submitting Jobs with `sbatch` \n",
    "To submit the above job scripts to Slurm, use the `sbatch` commands: \n",
    "\n",
    "```bash \n",
    "$ sbatch myjob.slurm\n",
    "Submitted batch job 123456\n",
    "```\n",
    "\n",
    "Slurm will respond with a job ID (in this example, `123456`). At this point, your job is in the queue. It might start immediately if resources are free or wait in line if the cluster is busy. \n",
    "\n",
    "Key points about `sbatch`:\n",
    "It queues the job, and then you **return to your shell prompt**. The job runs asynchronously in the background on the cluster. \n",
    "The `sbatch` command is non-interactive; it just submits the job. You won't see the job output on your screen live, it will go to the files defined by --output`/`--error` in the script,. \n",
    "\n",
    "## Checking Job Status with `squeue` \n",
    "\n",
    "Once a job is submitted, you'll want to check its status (queues, running, finished). Use `squeue` to view the job queue: \n",
    "\n",
    "```bash \n",
    "$ squeue -u your_username\n",
    "```\n",
    "\n",
    "This shows all your jobs (use `squeue` alone to see everyon's jobs, but that can be long on busy systems. Typical `squeue` output columns include: \n",
    "- **JOBID**: The job ID (e.g. 123456) \n",
    "- **PARTITION**: Which partition/queue is it in?.\n",
    "- **NAME**: the job name. \n",
    "- **USER**: who submitted it. \n",
    "- **ST**: state (R = running, PD = pending/waiting, CG = completing, F = finished, CA = canceled, etc.)\n",
    "- **TIME**: how long it's been running (or pending) \n",
    "- **NODES**: number of nodes allocated. \n",
    "- **NODELIST(REASON)**: which node(s) it's on, or the reason it's pending (e.g. resource, priority, etc.) \n",
    "\n",
    "For example, if your job is waiting, you might see `PD` and `REASON` might be \"Resources\", meaning it is waiting for resources to free up. \n",
    "\n",
    "```{note}\n",
    "You can also filter by job ID (`squeue -j 123456) or other criteria. Slurm has many options, but checking by username is simplest to see all your jobs. \n",
    "```\n",
    "\n",
    "## Canceling a job with `scancel`\n",
    "If you need to stop a job (maybe you realised there's a bug or its taking too long), you can cancel it: \n",
    "\n",
    "```bash \n",
    "$ scancel 123456\n",
    "```\n",
    "\n",
    "Replace `123456` with the job ID you want to cancel. This will terminate the job if it's running or remove it from the queue if it hasn't started yet. After cancelling, use `squeue` to verify it's gone or see it marked as CA (cancelled). \n",
    "\n",
    "You can cancel all your jobs with `scancel -u your_username`. You can also cancel an entire job array or a range of jobs if needed. \n",
    "\n",
    "## Job Dependencies: Ordering Jobs \n",
    "Slurm allows you to chain jobs so that one doesn't start until the other is complete (and, optionally, only if it succeeds). This is done with the `--dependency` option of `sbatch`. \n",
    "\n",
    "**Use case**: Suppose you have two jobs, and *Job2* should run only after *Job1* finishes successfully. It could be the case that Job1 generates data that Job2 will process. You can submit Job1 normally, then submit Job2 with a dependency on Job1. \n",
    "\n",
    "Submit the first job and note its job ID: \n",
    "\n",
    "```bash \n",
    "$ sbatch job1.slurm\n",
    "Submitted batch job 111111\n",
    "```\n",
    "\n",
    "Submit the second job with dependency: \n",
    "\n",
    "```bash \n",
    "$ sbatch --dependency=afterok:111111 job2.slurm\n",
    "Submitted batch job 111112\n",
    "```\n",
    "\n",
    "The `dependency=afterok:111111` means \"run this job after 111111 finished *after* OK (exit code 0).\" In other words, job2 will wait until job1 is done *successfully*. If job1 fails (non-zero exit), job2 will not run (it will be canceled due to dependency failure). \n",
    "\n",
    "There are a number of other dependency types, including: \n",
    "- `afterany:<jobid>`: run after job finishes regardless of success or failure.\n",
    "- `after:<jobid>`: run after job starts (not commonly used; `afterok` is more typical).\n",
    "- `singleton`: ensure only one job with the same name/user runs at a time (to avoid duplicates).\n",
    "You can chain multiple job IDs like `--dependency=afterok:ID1:ID2` (job runs after both ID1 and ID2 succeed).\n",
    "\n",
    "Dependencies are powerful for building task pipelines. For instance, you could have a preprocessing job, then a training job, then a post-processing job, each submitted with appropriate `--dependency` so they execute in sequence without manual intervention. \n",
    "\n",
    "### Interactive Jobs (srun)\n",
    "While `sbatch` is for batch submission, Slurm also has `srun` for running tasks interactively (especially useful for debugging or running short tests on compute nodes). For example, `srun --pty bash` will give you an interactive shell on a compute note. This course focuses on batch jobs for GPU tasks, but keep in mind that `srun` exists for interactive use. \n",
    "\n",
    "## Practical Tips for Slurm \n",
    "- **Default Behaviour**: If you don't specify an output file, Slurm, by default, writes output to a file like `slurm-<jobid>.out`. It's better to set `--output` to something meaningful. \n",
    "- **Resource Requests**: Always request resources (time, memory, GPUs) realistically. If you ask for too little, your job might be killed for exceeding memory or time. If you ask for too much, you could wait longer in the queue. \n",
    "- **Partition/Queues**: Clusters often have multiple partitions (e.g. `GPU`, `CPU`, `long`, `debug`). Make sure to use an appropriate one, as each has limits (a debug queue might only allow short 30-minute jobs, for example). \n",
    "- **Monitoring**: You can monitor usage on a running job with commands like `sstat` (for stats) or by logging into the node (if interactive) and using tools like `nvidia-smi` to see GPU usage. \n",
    "\n",
    "## Summary: Key Slurm Commands\n",
    "- `sbatch <script>`: Submit a job script to the queue.\n",
    "- `squeue`: Check job status in the queue (use `-u <user>` to filter by your username).\n",
    "- `scancel <jobid>`: Cancel a job.\n",
    "- **Dependencies**: Use `sbatch --dependency=afterok:<jobid>` to chain jobs.\n",
    "- **Interactive**: `srun` for interactive jobs (not covered in depth here, but useful to know).\n",
    "\n",
    "## Exercise\n",
    "---- Submitting a premade script that will verify that everything is set up, e.g. the number of CUDA cores, use the spack environment they made earlier etc----\n",
    "\n",
    "## Quiz\n",
    "---- Ask questions about the script itself, such as what is being requested what partition it is being ran on  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2717b-1fc4-40ee-aaf6-b429dafc59a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
